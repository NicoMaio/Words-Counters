Scheduling: Meccanismo del SO che decide qual è il prossimo thread da far mettere in esecuzione dal processore. Per svolgere ciò lo scheduling deve agire in diversi momenti e spesso viene invocato da varie parti del So quando si svolge una modifica dello stato dei thread, modifica delle code di attesa etc. 
Def Task/Job: Azioni più brevi rispetto allo scheduling svolte nel singolo thread. Task, per esempio, può essere la reazione al movimento del mouse, che non completa l’attività del thread che regola il movimento del mouse, ma sono delle attività interattive che necessitano di un’immediata risposta a seguito degli stimoli esterni. Job: s’intende un Task non interattivo. Ovvero è un’attività che quando viene messa in esecuzione dispone già di tutte le risorse necessarie per arrivare al completamento. Un Job può essere implementato a livello di thread, a livello di processo, a livello di processo multithread. Dunque un Job non è un’attività interattiva e ciò vuol dire che una volta messo in esecuzione il Job avrà solo bisogno che gli si venga messo il processore a disposizione, non avrà bisogno di attendere ulteriori risorse. Latenza/tempo di risposta: La latenza è il tempo che passa fra l’istante in cui viene dato un comando e l’istante in cui l’operazione indicata dal comando viene completata. Per esempio esecuzione di un certo programma da linea di comando, la latenza la si misura con la differenza fra il tempo di completamento dell’esecuzione del programma e l’istante in cui è stato mandato in esecuzione. Oppure intervallo di tempo fra movimento del mouse fisico e visualizzazione risultato. Throughput: misura la quantità di lavoro che si riesce a svolgere per unità di tempo. Se siamo in un contesto di processi interattivi, dove i tempi di esecuzione dipendono dai tempi dell’utente, parlare di throughput non ha molto senso. Se siamo in un contesto di un server o di un sistema bash che non interagisce con l’utente, in questo caso i tempi di esecuzione sono legati alla velocità del processore, alla velocità dei dispositivi e alla capacità dello scheduler di riuscire a gestire in modo ottimo lo scheduling dei thread che devono essere eseguiti. Quindi in questo contesto ha senso parlare di throughput e viene misurato in numero di Job che possono essere completati dal sistema per unità di tempo o numero di processi che arrivano a completamento per unità di tempo etc. Dunque throughput è una misura prestazionale e ha senso valutarla per lo scheduler riguardo a processi che non sono interattivi come job. Overhead: riguarda la quantità di lavoro aggiuntivo che bisogna fare per mettere il sistema in grado di funzionare, rappresenta inoltre un tempo aggiuntivo che va a rallentare l’esecuzione dei thread. Nel caso dello scheduler l’overhead è tutto ciò che comporta l’attività di scheduling. Dato che l’obiettivo dello scheduling è di far funzionare il SO al meglio. Ma di per sé lo scheduler non produce alcun risultato che l’utente può percepire. Abbiamo esigenza di eseguire attività di scheduling per far funzionare il sistema e queste attività di scheduling devono essere eseguite sullo stesso processore sul quale vengono eseguiti gli altri processi. Dunque tanto più lo scheduler è efficiente, tanto più il suo tempo di esecuzione sul processore e gli effetti di rallentamento dovuti allo scheduler dei processi, sono minimi, dunque tanto più il suo overhead è basso. Dal punto di vista dello scheduling l’overhead non riguarda solo il tempo di esecuzione dell’algoritmo di scheduling sul processore, ma riguarda anche gli effetti impliciti dello scheduling. Se lo scheduling è di tipo con pre-rilascio (può forzare il processore a fare commutazione di contesto), causerebbe una commutazione di contesto che è parte del overhead. Dunque è importante che lo scheduler riduca sia il suo overhead esplicito ovvero il tempo necessario per prendere decisioni sia overhead implicito con rallentamenti che derivano dalle sue decisioni. Tra le due tipologie di overhead il secondo è quello più caro. Il primo è rappresentato da un costo abbastanza ridotto. Fairness (equità) lo scheduling esiste perché nel nostro sistema abbiamo a che fare con più thread, più processi e tal volta con più utenti; ma grazie allo scheduler il sistema riesce ad offrire un servizio equo e simile a diversi utenti o a diversi thread. Dunque il parametro equità è un parametro di qualità vorremmo che il nostro scheduler sia il più equo possibile. Predicibilità: predicibilità dell’esecuzione di certi compiti. Un utente che interagisce con il sistema si aspetta che ad esempio il movimento del mouse sia instantaneo con il movimento del cursore. In realtà non è così passa del tempo fra i due eventi, ma dal punto di vista dell’utente è importante che tale intervallo sia quasi nullo. Non è sempre detto che ciò avvenga e in alcuni casi possibili ritardi possono essere causati anche dallo scheduler che ha preferito far svolgere degli altri task al processore piuttosto che quello del mouse. La predicibilità riguarda sia alcuni task molto piccoli o anche marginali (come il movimento del mouse), sia task molto più complessi come l’esecuzione di programmi molto costosi dei quali vogliamo che le diverse esecuzioni durino lo stesso intervallo di tempo. 
L’insieme di tutti i thread o task che devono essere messi in esecuzione o che sono oggetti di scheduling, viene definito workload ovvero il carico di lavoro sul quale lo scheduling deve lavorare. Lo scheduling può essere con pre rilascio e work-conserving. Il fatto che lo scheduler sia con prerilascio o meno dipende dal modo con il quale è stato realizzato per quel sistema e ciò spesso dipende anche dal supporto hardware che il processore stesso fornisce alle attività di scheduling. Uno scheduling con pre rilascio significa dare la possibilità al SO (allo scheduler) di intervenire in qualsiasi momento, arbitrariamente, per forzare il rilascio del processore da parte di un thread che è attualmente in esecuzione. I sistemi moderni sono tutti basati su pre rilascio. Con uno scheduler con pre rilascio il sistema può essere ottimizzato e si possono garantire una serie di proprietà in più, ma necessita di un maggior supporto da parte dell’hardware. Per quest’ultimo motivo alcuni sistemi come quelli più primitivi non prevedono pre rilascio perché non offrono un supporto adeguato per poterlo gestire. Per poter gestire pre rilascio bisogna disporre di un timer, meccanismo che faciliti la commutazione di contesto. Lo scheduling deve essere work-conserving ovvero che cerchi di mantenere il processore impegnato per più tempo possibile. Di solito abbiamo a che fare con scheduling con work-conserving dunque se ci sono dei task o dei thread pronti per essere messi in esecuzione, vengono messi in esecuzione e il processore non viene lasciato inattivo. I sistemi che non sono work-conserving sono molto particolari come ad esempio i sistemi che devono fare efficienza energetica non sono spesso work-conserving ma spesso sfruttano i periodi di attesa del processore per abbassare i consumi energetici. Infine vi è l’algoritmo di scheduling che prende in ingresso un work-load (insieme di thread in attesa di essere eseguiti, in stato di pronto), decide, fra i thread in work-load, quale deve essere allocato sul processore, e come effetto da questa decisione, ci sono dei parametri prestazionali che possiamo utilizzare per verificare quanto sia buono l’algoritmo di scheduling (throughput e latenza).
Soluzione FIFO: in passato (molto passato ed anche abbandonato quasi subito) venne utilizzato uno scheduler basato sulla FIFO, e ne venne fuori uno scheduler abbastanza semplice. Dato un insieme di thread in stato di pronto, lo scheduler manda in esecuzione il primo arrivato. Tale tecnica viene usata in molti contesti (esempio code supermercato). I FIFO non è un tipo di scheduling con pre rilascio, normalmente è privo di pre rilascio. Il thread in esecuzione viene lasciato lì fin quando non termina oppure non cede spontaneamente il processore. Se termina il thread lo scheduler prende il prossimo thread in attesa nella lista e lo mette in esecuzione. Se invece il thread in esecuzione cede spontaneamente il processore, allora lo scheduler può intervenire e mettere in esecuzione il prossimo. Quest’ultimo evento può avvenire tramite yield (con la quale si simula un prerilascio in un sistema senza prerilascio il tutto gestito dal programmatore) oppure il thread in esecuzione è possibile che debba invocare delle operazioni che lo portano a bloccarsi (operazione di I/O molto lunga, o sincro con altri thread), in questo caso vi è sempre un meccanismo senza pre rilascio dato che è il thread che ha scelto di eseguire operazioni che lo ha portato a bloccarsi e se si è bloccato viene ceduto il processore e viene rimesso in esecuzione lo scheduler che manderà in esecuzione il thread successivo. Ma questa tipologia di comportamenti rientrano nell’ambito di scheluding senza pre rilascio. Il FIFO funziona male se i thread hanno lunghezza differente (esempio supermercato devo comprare solo il pane ma in fila prima di me ci sono alla cassa persone con carrelli stracolmi). Venne abbandonato a favore di SJF. SJF la forma con pre rilascio prende il nome di shortest remeaning time first, la forma senza pre rilascio si chiama SJF (Shortest Job First): se sono presenti task di dimensioni differenti, do la priorità ai task brevi, in particolare al task più breve di tutti e poi metto in esecuzione i task più lunghi progressivamente. Nella versione senza pre rilascio avviene ciò che è stato appena descritto. Nella versione con pre rilascio, invece, non prendo la durata dei task, ma la durata residua dei task ovvero vado a vedere di ogni task quanto tempo gli manca al completamento, li ordino sulla base di ciò e li schedulo su questa base. Se un thread viene messo in esecuzione, ovvero aveva il tempo più breve degli altri, e fa un’operazione di I/O, rilascia il processore, entra in esecuzione un altro thread che però aveva un tempo di completamento più lungo, ma resta in esecuzione finchè l’operazione di I/O non si completa e a questo punto il tempo di esecuzione residuo può essere diventato più breve o più lungo rispetto al thread che c’era prima in esecuzione. Dunque una volta completata la I/O lo scheduler prende una decisione riguardo a quale sia fra i thread disponibili quello con tempo residuo minore per metterlo in esecuzione, può essere quello che c’era all’inizio che ha richiesto l’I/O oppure quello che è stato messo in esecuzione subito dopo che si è accorciato durante il suo tempo di esecuzione e in questo caso si nota l’effetto di Shortest remaining time first. Dunque lo scheduler può intervenire ogni qual volta si rende conto che sia disponibile un thread con tempo residuo inferiore. Esempio task slide 09 di set 07. Nell’esempio si parla di task perché quando abbiamo a che fare con SJF ci serve un parametro da associare, ovvero ci deve essere una stima del tempo richiesto per l’esecuzione o per il completamento di ogni task. In un sistema moderno con thread e processi, tale stima è molto difficile da fare se il programma è molto complesso e addirittura impossibile se il programma è addirittura interattivo. SJF è nato in un contesto nel quale i sistemi erano di tipo batch dunque ciò che si schedulava non erano processi o thread erano in realtà job. Dunque sono necessari processi non interattivi che dispongono già di tutti i dati, per poter arrivare a completamento quando li si mette in esecuzione. Questa tipologia di lavori, è possibile fare una stima preliminare sui tempi di esecuzione, ed è in questo contesto che l’SJF è nato. Utilizzare SJF in un contesto moderno con thread praticamente quasi tutti interattivi, non è più pensabile.
Differenza fra task e thread. Un thread è un’attività in esecuzione controllata da un programma. In questa forma un thread può portare avanti esecuzione di un task. In realtà un task può essere implementato da un singolo thread o da più thread, può essere un processo multi thread o può essere un più processi multi thread. Un task è genericamente un’attività che deve essere completata e può essere svolta da un singolo thread o meno. La cosa importante è che del job dobbiamo poter stimare il tempo di esecuzione quindi non deve essere interattivo, e quando lo mettiamo in esecuzione dobbiamo avere già tutte le info che gli serviranno per arrivare a completamento. Mentre un job è necessariamente un task non interattivo, un task potrebbe essere qualsiasi cosa; i thread sono gli strumenti usati per implementare tali attività.
Considerazioni su SJF: L’SJF è ottimo dal punto di vista di tempo di risposta medio. Il tempo di risposta è il tempo che intercorre tra l’istante in cui di quel particolare task ne viene richiesta l’esecuzione a sistema e l’istante in cui quello specifico task completa la sua esecuzione. Tale tempo di completamento per un task include sia il tempo effettivo di esecuzione sul processore, sia il tempo di attesa nelle code. Se uso SJF il tempo di completamento medio dei task è minimizzato (proprio minimo con SJF si raggiunge ottimo dal punto di vista di tempo di completamento medio).
Limiti su SJF: se siamo in un contesto in cui i job arrivano tutti nello stesso momento, vengono ordinati, schedulati e successivamente arrivano i risultati allora l’SJF va benissimo così (scenario anni 50 con job che arrivano tutti la mattina e risultato la sera). In uno scenario più moderno in realtà i job possono arrivare in qualunque momento e se uso SJF (in forma più dinamica), e ho un job molto lungo, può succedere che il job vada in starvation ovvero in attesa indefinita nel caso in cui arrivano lavori sempre più brevi di lui, gli passano davanti e il job molto lungo dovrà aspettare. L’attesa indefinita non vuol dire che quel job non verrà mai completato, il problema è che non si può fare una previsione sul suo tempo di completamento e potrebbe aspettare un tempo indefinito ma non infinito. Tutto ciò va a discapito del SO e va a discapito dell’utente.
Considerazioni su FIFO: il FIFO non può essere meglio di SJF, esso però è vicino all’ottimo quando i task hanno esattamente la stesa lunghezza e il FIFO diventa quasi uguale a SJF.  
Esempio SJF per cui è considerato ottimo. Immaginando di avere 4 job grandi A,B,C,D che hanno tempo di esecuzione a,b,c,d piccoli; vengono schedulati in ordine alfabetico. Il tempo di completamento per A grande è semplicemente a ovvero l’intervallo che passa tra il termine del job A e l’istante in cui il job A è stato messo in esecuzione. Ipotizzando che lo scheduling abbia tempo trascurabile, nel momento in cui A arriva viene messo in esecuzione, e la sua esecuzione dura esattamente a. Il job B arriva con A ma viene messo in esecuzione solo dopo che A è completato. Quindi il suo turn around è pari ad a+b, b è il tempo in cui è stato effettivamente in esecuzione sul processore, e a è stato il tempo in cui il job B è stato in attesa sulle code. Per il job C esso arriva insieme ad A ma viene messo in esecuzione dopo che A e B sono completati. Quindi C è stato un tempo a+b in attesa sulle code e un tempo c in esecuzione. Anche D sta sulla coda pronti per un tempo a+b+c e in esecuzione per un tempo d. Dunque il risultato è che se vado a vedere il tempo di completamento medio, esso è dato dalla somma del completamento dei 4 task diviso 4. Quindi possiamo tralasciare il diviso 4 perché è una costante, e posso guardare il totale che è 4a+3b+2c+d. Se devo ottimizzare ciò, ovvero andando a trovare il minimo, scopro che esso lo raggiungo quando sono ordinati ovvero quando sono nell’ordine a<b<c<d. E non è possibile che un ordinamento differente minimizzi il tempo turn around totale. Questo perché a viene moltiplicato per 4 e perciò deve essere il più piccolo. 
Il risultato è che l’SJF è stato adottato in sistemi sostanzialmente batch sia nella versione con pre rilascio che nella versione senza pre rilascio, e poi ha dimostrato i suoi limiti soprattutto dal punto di vista della starvation e della variazione nel tempo di risposta. La starvation è abbastanza grave perché non rende predicibili i tempi di completamento di un processo. La variazione dei tempi di risposta sta nel fatto che oscillano notevolmente i tempi di completamento. 
Round Robin meglio di SJF è arrivato per due motivi. Il primo per la necessità di superare i limiti delle SJF anche se si resta in un contesto di processi non interattivi, di sistema batch, questi due motivi sono sufficienti per spingerci a cercare un sistema che sia più equo soprattutto nei confronti dei lavori lunghi, in un contesto più dinamico. Il secondo motivo è che un approccio all’SJF è pensabile solo su sistemi batch, su sistemi interattivi non funziona perché abbiamo altri problemi a parte ottimizzare uso del processore, ma abbiamo anche il problema di ottimizzare il tempo di risposta e la soddisfazione degli utenti. Quindi vanno introdotti anche degli altri elementi nella valutazione delle prestazioni; non si va più a cercare una valutazione pura come nell’SJF. Così nacque il Round Robin. Idea: si divide il tempo del processore in slot temporali e si assegna il processore al termine di ogni slot temporale. Quando il processore viene assegnato, il processo che viene messo in esecuzione, resta in esecuzione per tutto quello slot senza essere interrotto. Quando lo slot termina, il SO riacquisisce il controllo, mette in esecuzione lo scheduler che seleziona il prossimo processo da mandare in esecuzione e la selezione avviene a turno. Tutti i processi vengono messi ordinati in una coda circolare, il processo da mettere in esecuzione viene messo in testa e quando esso ha completato il suo quanto di tempo viene messo in coda. Così facendo il sistema diventa più equo perché a tutti i processi diamo la stessa possibilità di andare avanti e ciò rende migliore il sistema multitasking. 
Introducendo il Round Robin vengono introdotte delle altre problematiche da considerare. Perché nel caso di SJF e FIFO in realtà per essere ottimizzati sono fantastici dato che non hanno parametri e dunque non c’è nulla da ottimizzare. Nel caso del Round Robin, è stato introdotto un elemento aggiuntivo che è il quanto di tempo, e questo elemento va configurato. Dunque il problema diventa: quant’è la durata giusta per il quanto di tempo? Che valore devo scegliere per il quanto di tempo? 
Se il quanto di tempo è troppo lungo, se il thread che ho messo in esecuzione in questo quanto di tempo termina, in realtà non resta un pezzo di quanto di tempo con tempo morto, ma in realtà il thread che termina restituisce il quanto di tempo che gli restava. Quindi quando un thread passa in esecuzione per un quanto di tempo, se termina in mezzo, termina, interviene lo scheduler, e i quanti di tempo ripartono da questo istante dunque non ci sono tempi morti. 
Se il thread invoca operazione di I/O durante il suo quanto di tempo, e quindi si blocca, oppure se il thread fa una P su semaforo a zero e quindi si blocca, nuovamente viene rinvocato lo scheduler in quell’istante e quindi questi slot temporali non sono sempre allineati tutti nello stesso modo, ma se per caso un thread rilascia preventivamente il processore, si riallineano tutti a quel termine lì. Quindi il quanto di tempo è il limite superiore di tempo d’esecuzione del thread (Upper Bound). 
Dunque se il quanto di tempo è troppo lungo il Round Robin diventa un FIFO, dunque più si allunga il quanto più il Round Robin diventa simile al FIFO. [Il problema sta nell’overhead. Ogni qualvolta che interviene lo scheduler alla fine del quanto di tempo e fa una commutazione di contesto, tale commutazione ha un overhead molto grande sia per salvare il thread in esecuzione sia per ripristinare il contesto del thread che metto in esecuzione, in più vi è l’overhead che comprende il passaggio dallo stato utente allo stato supervisore e la gestione dell’interrupt e in più anche tutto l’overhead legato al fatto che il thread che viene messo in esecuzione potrebbe appartenere ad un altro processo e quindi c’è il problema dell’invalidazione delle cache del processore etc.] Quindi con un quanto di tempo troppo breve, tale overhead che pago ad ogni commutazione di contesto, lo pago molto spesso. È il sistema che perde una frazione di tempo non indifferente per fare cambio di contesto. Dunque il quanto di tempo deve essere una via di mezzo né troppo lungo per avere i vantaggi del FIFO né troppo breve per poter ammortizzare l’overhead e renderlo trascurabile rispetto all’avanzamento dei thread ovvero rispetto all’effettivo tempo di esecuzione dei thread.
Esempio se prendo quanto di tempo molto lungo, il thread più lungo impiega tutto il quanto e i thread molto brevi ne usano una piccola frazione e allungando sempre di più il quanto di tempo il Round Robin diventerà un FIFO. Invece se prendo quanto di tempo molto breve, ovvero pari al tempo di esecuzione dei thread più piccoli, in questo caso il Round Robin diventa equivalente a SJF. Esempio slide 07 pagina 14.
Round Robin fatto come nell’esempio della slide 07-14 garantisce alcune proprietà come: la proporzionalità ovvero il tempo che un task spende nel sistema per la sua esecuzione, il suo tempo di completamento, nel caso del Round Robin è effettivamente proporzionale alla lunghezza del task non pari, e il fattore di proporzionalità dipende da quanto sia carico il sistema. Tanto più il sistema è carico tanto più tale costante di proporzionalità sarà grande. Se in sistema sono presenti pochi job tale costante di proporzionalità si ridurrà. Il tempo di risposta nel caso del Round Robin si può stimare abbastanza bene perché dipende da quanti thread stanno in coda. Nel caso peggiore la risposta ci arriva non appea siamo entrati in coda, e appena la coda si svuota siamo tornati in esecuzione. Quindi con il Round Robin abbiamo anche un Upper Bound ai tempi di risposta sui singoli stimoli. Se premo un tasto sulla tastiera, il thread che dovrà recepirlo, lo recepirà e farà la visualizzazione in un tempo che è pari, nel caso peggiore al numero dei thread in coda pronti moltiplicato per il quanto di tempo, dunque dipende da quanto sia carico il sistema.
Round Robin in caso di attesa: Se un thread A è in esecuzione durante il suo quanto di tempo esegue una wait su una variabile di condizione e si sospende. A va in stato di attesa, e istantaneamente lo scheduler viene invocato e viene messo in esecuzione B. In un momento successivo A viene riattivato, ma quando viene riattivato, verrà messo in fondo alla coda non in cima per questioni di equità rispetto agli altri thread che erano presenti in coda.
Meccanismi per realizzare Round Robin. Ci serve un timer che segni la scadenza dei quanti di tempo e deve essere fornito dall’HW. Quando il timer scatta, deve fornire un’interruzione che deve essere riservata allo scheduler. Il timer HW viene intercettato dallo scheduler. Se nel SO vi deve essere un conteggio del tempo, lo stesso timer dell’interrupt farà entrambe le cose. Verrà settato con intervalli per l’orologio e con intervalli per lo scheduler e ciò è facilmente gestibile virtualizzando il timer. Lo scheduler deve anche entrare in esecuzione ogni qualvolta viene modificato lo stato dei processi. Se un processo si sospende: si chiama lo scheduler; se un processo termina: si chiama lo scheduler; se un nuovo processo viene messo in esecuzione: si chiama lo scheduler; etc. Lo scheduler riassegna il processore se è il caso di farlo e rimposta il timer, dunque il quanto di tempo riparte dall’istante in cui viene attuata tale modifica. Nei sistemi attuali il quanto di tempo dipende, alcuni usano più di una misura per il quanto di tempo a seconda dei processi che stanno eseguendo. Idealmente varia dalle decine alle centinaia di milli secondi. Come tra i 20-100 ms. C’è tale differenza perché i sistemi non sono tutti uguali. Se un sistema è molto interattivo, è meglio utilizzare quanti di tempo più brevi, perché nonostante penalizzino i thread più lunghi diminuiscono i tempi di risposta. Nel caso di sistemi server dove la parte di interazione con utente è marginale e c’è da svolgere dei calcoli e da rispondere in continuazione alle richieste che arrivano da più fronti, allora in tali casi conviene avere un quanto di tempo più lungo, quindi i server hanno quanti di tempo più lunghi.
Round Robin sempre meglio di FIFO? (senza overhead) Se abbiamo dei thread tutti quanti di una durata <= al quanto di tempo il FIFO e il Round Robin sono identici. Tralasciando questo caso c’è un caso in cui il FIFO funziona meglio del Round Robin. Risposta esempio slide 07-18. I sistemi operativi più complessi devono essere misurati con una quantità innumerevole di parametri ed è difficile che una soluzione vada bene per tutti fra (quanto di grande dimensioni e quanto di breve dimensioni) quindi molto spesso bisogna trovare dei punti di equilibrio.
Round Robin è sempre equo? No, i processi o thread non sono tutti quanti uguali, essi possono oscillare fra due estremi: processi CPU-Bound e processi I/O-Bound. Non sono processi interattivi. I processi CPU-Bound sono i processi limitati dalla CPU ovvero il loro tempo di esecuzione è limitato dalla velocità della CPU perché questo processo svolge la maggior parte dei suoi compiti sul processore (come processi che devono fare calcolo intensivi). Invece un processo con comportamento I/O-Bound vuol dire che il processo è limitato dalla velocità I/O ovvero limitato dalla velocità dei dispositivi. Un processo che fa spesso accesso ai dispositivi è un esempio di processo I/O-Bound. Ad esempio un processo che fa la copia di un file sulla CPU fa quasi nulla. Questi tipi di processi fanno pochi calcoli quindi stanno poco in CPU ma fanno molte operazioni di I/O per cui la loro effettiva velocità d’esecuzione è legata alla velocità dei dispositivi. Un sotto caso dei processi I/O-Bound sono i processi interattivi. In quest’ultimo caso il dispositivo è l’essere umano. Quindi sono limitati dalla velocità con la quale l’essere umano li stimola. Esempio 07-21. Il Round Robin è equo se i processi sono equivalenti ovvero hanno le stesse caratteristiche, se invece siamo in condizione in cui i processi hanno natura e caratteristiche molto differenti, in particolare CPU-Bound, I/O-Bound, i processi I/O Bound sono fortemente penalizzati e in queste condizioni il Round Robin non è più equo.
Per risolvere tale problematica abbiamo bisogno di un meccanismo che viene detto Max-Min Fairness quindi cercare di massimizzare la quantità minima di tempo che viene data ad ogni processo. In questo caso si possono adottare diverse soluzioni come la MFQ (Multi-level Feedback Queue) o ad esempio nel caso di windows si utilizzano una variante di MFQ. Altri meccanismi sono le code di priorità che si adottano in Linux o altre cose del genere.
MFQ Obiettivi: mantenere tempi di risposta brevi, se possibile migliorare rispetto a Round Robin; mantenere un basso overhead; non avere attesa indefinita e per questo motivo si introduce una priorità ovvero le code sono associate a priorità diverse, andando a schedulare un thread che sta nella coda più alta in Round Robin, stiamo dicendo che la coda più alta ha priorità maggiore; si cerca di mantenere equità fra task che sono all’interno della stessa coda. Cercando di migliorare tutto ciò la MFQ non fa bene niente. Quindi se prendo un parametro di tempo di completamento medio non sarà meglio di SJF. E anche in certi casi particolari il FIFO funziona meglio. Il problema è che cercando di risolvere tutti i problemi si usa una soluzione intermedia che sebbene non sia perfetta su tutti i parametri, fa mediamente bene su tutti i parametri, sono le soluzioni adottate dai sistemi operativi principali. Idea: In questo caso non abbiamo più un’unica coda di processi pronti ma abbiamo tante code di processi pronti. All’interno di ogni coda facciamo uno scheduling di tipo Round Robin. I processi si possono accodare su una qualsiasi di queste code. Inizialmente vengono messi sulla coda di priorità maggiore. Lo scheduler guarda la prima coda, se è vuota va alla seconda, e poi alla terza etc. quando trova una coda non vuota prende il primo processo dalla coda e lo mette in esecuzione, operando in maniera Round Robin. Se c’è un processo su una coda più alta si sposta su di essa e fa Round Robin su di essa. I thread che fanno parte di coda con priorità maggiore hanno quanto di tempo più breve, i thread che fanno parte della coda con priorità minore hanno quanto di tempo più lungo. Il meccanismo funziona bene se siamo in grado di mantenere i thread I/O Bound nelle code di priorità maggiore e i thread CPU Bound nelle code di priorità minore. In queste condizioni succede che i thread I/O Bound nelle code di priorità minore avranno priorità e passeranno subito in esecuzione con un quanto di tempo molto breve. I processi CPU Bound andranno in esecuzione soltanto se non sono presenti thread nelle code di priorità maggiore ovvero se i thread I/O Bound sono in attesa, cosa ampiamente possibile dato che passano la maggior parte del loro quanto in attesa, così verranno eseguiti i thread CPU Bound con quanti di tempo maggiore e sfrutteranno processore senza grosse interruzioni. 
Come facciamo a mettere thread I/O Bound in cima e thread CPU Bound in fondo?  Ci sono diverse questioni: non si può chiedere di farlo fare all’utente, e inoltre un processo se nasce I/O (CPU) Bound non è detto che resti sempre I/O (CPU) Bound. Esempio programma elaborazione video, nella fase in cui si aspetta i tagli, incolla dell’utente il processo è solo I/O Bound, nella fase di transcodifica del video il processo diventa completamente CPU Bound perché deve fare un sacco di calcoli. Dopo di che, svolti questi calcoli, ridiviene un I/O Bound. 
In MFQ schedulo i thread all’interno della coda di priorità maggiore con Round Robin, quando il thread ha terminato il suo quanto di tempo, quindi ho usato per intero il suo quanto di tempo allora viene scalato nella coda inferiore. Se il thread continua ad utilizzare sempre per intero il suo quanto di tempo è evidentemente un CPU Bound. Perché se faccio l’I/O interrompe prima di completarlo il suo quanto di tempo. Dunque questo meccanismo fa in modo che i CPU Bound tendano ad andare col tempo sempre più verso il fondo e i processi I/O Bound restano sempre in alto. Quando il processo viene messo in esecuzione la prima volta, parte sempre dalla coda di priorità maggiore. 
Considerazioni: Attesa indefinitia. L’MFQ non elimina il problema dell’attesa indefinita. Nel Round Robin era eliminato, l’MFQ introducendo un concetto di priorità, di nuovo introduce un problema che era stato rimosso. Se continuano ad arrivare processi I/O Bound, e ce ne sono tanti per cui saturano il processore, i processi CPU Bound vanno in attesa indefinita. Dunque tale meccanismo usato da solo in modo così rigido non basta. Cambiare Abitudini. L’MFQ possono filtrare e mandare a fondo i processi CPU Bound, però il problema sta nel fatto che i processi possono cambiare nel tempo i loro comportamenti e anche più volte. Quindi avrò bisogno non solo di mandare a fondo i CPU Bound ma anche di far galleggiare i CPU Bound che nel frattempo sono diventati I/O Bound. Quindi ho bisogno di combinare la tecnica delle code multiple con dei meccanismi che mi permettano di fare ciò ovvero di variare le priorità in maniera dinamica sulla base dei comportamenti effettivi del processo. Un esempio di come ciò viene fatto è il caso di Windows. Esempio slide 07 – 27.
Scheduling su windows da esempio 07-27. Per evitare starvation dei thread più bassi ovvero dei thread utente. In realtà tale starvation non si sviluppa perché i thread del nucleo sono thread che svolgono un servizio per i thread utente. Se non c’è nessun thread utente che passa in esecuzione dopo un po’ non ci saranno più richieste di servizio. Quindi non ci sarà più niente da fare per i thread nel nucleo quindi non ci saranno più thread del nucleo in stato di pronto. Si dà più priorità ai thread nel nucleo perché essi svolgono un servizio per un thread utente che evidentemente sta aspettando e quindi lo vogliono far aspettare il meno possibile. Per questo motivo si dà la priorità ai thread nel nucleo così finiscono prima e sbloccano prima gli altri thread che stanno in attesa, dunque non è un problema per la starvation. Scheduling a livello utente. I thread quando vengono creati partono da un livello di priorità pari a 8. E quindi di base fra di loro lo scheduling è Round Robin. Dopo di che Windows utilizza dei meccanismi di variazione dinamica della priorità per abbassare o alzare la priorità. Incremento di priorità. Se un thread si sospende e viene riattivato e si era sospeso per un’operazione su un dispositivo multi materiale, come su audio oppure sulla tastiera, allora prende un incremento che è tanto maggiore tanto più quel dispositivo ha caratteristiche di multimedialità o di interazione. Quindi per esempio se il thread si è sospeso sulla scheda audio, quando viene riattivato prende un incremento di priorità pari a +8, se si è sospeso su un’operazione della tastiera prende un incremento pari a +8, se era invece un’operazione su una linea seriale +6, se era su disco +1. Il risultato è che il thread che si è sospeso su un’operazione di I/O vede incrementata alla sua riattivazione la sua priorità. E questo perché si è fatta un’operazione che potrebbe essere I/O Bound, anche se non è detto che lo sia. Tale incremento di priorità non vale per sempre. Vale per un paio di cicli di Round Robin. Quindi il thread che ottiene tale incremento mantiene la priorità per un paio di quanti di tempo e poi la sua priorità ritorna a livello normale. Se questo thread è I/O Bound probabilmente durante questi quanti di tempo richiederà nuovamente delle operazioni di I/O e quindi confermerà l’incremento. Tale incremento arriverà al massimo a 15. Se il thread è in particolare anche interattivo ovviamente il suo incremento di priorità è maggiore perché deve passare in esecuzione prima. Inoltre c’è il meccanismo di far affondare i thread che sono CPU Bound. Ciò avviene come in MFQ ovvero se un thread finisce tutto il quanto di tempo la priorità viene scalata e inizia a scendere. Quindi si è trovato meccanismo per far galleggiare I/O Bound e far affondare CPU Bound. Se per caso un CPu Bound ridiventa I/O Bound inizia a fare operazioni di I/O e nel giro di breve tempo la sua priorità sale e se diventa interattivo la sua priorità sale ancora più rapidamente. Dal punto di vista della starvation se un processo è CPU Bound e resta CPU Bound potrebbe non tornare in esecuzione se ci sono molti I/O Bound. Per questo motivo c’è il meccanismo per cui se un thread non è stato in esecuzione per un tempo molto lungo, allora la sua priorità viene elevata a +15 per un paio di quanti di tempo e in questa maniera dovrebbe recuperare il tempo perduto e non ci può essere starvation perché nel caso peggiore, quando scade il timer viene messo nella coda di priorità maggiore e dopo un ciclo di Round Robin passerà in esecuzione e riuscirà ad andare avanti. 
Fenomeno reversione delle priorità. Supponendo di avere due thread: un produttore e un consumatore. Esempio slide 07-29.  Supponendo che questi thread abbiano un livello di priorità differente, per esempio il thread A che potrebbe essere il nostro consumatore ha priorità 12 mentre il suo produttore potrebbe essere B che ha priorità 5. Se il buffer è vuoto A aspetterà B. E B dopo che ha prodotto risveglierà A. Dato che ci sono due priorità differenti può succedere che ci sia in mezzo un altro processo C che non ha niente a che vedere con A e con B e non ha priorità intermedia. In queste condizioni A è sospeso, B è l'unico che può sbloccare A ma non può farlo perché c'è in esecuzione in continuazione C che ha priorità 8 e resta lì e non gli dà spazio. Se lo guardiamo così ha senso ma se lo guardiamo in generale C sta bloccando un thread con priorità maggiore della sua ovvero A. E ciò è definito fenomeno dell'inversione di priorità. Per rimediare a questo problema è stato introdotto il meccanismo di innalzare la priorità di thread che restano in attesa troppo a lungo. Con questo meccanismo il thread B dopo un po’ passa per forza in esecuzione e a quel punto è in grado di sbloccare A. 
Dunque anche in questo caso possiamo dare dei quanti di tempo più brevi alle code più alte e dei quanti di tempo più lunghi alle code più basse.
Riassumendo. Tutti gli algoritmi di scheduling visti fanno operazioni abbastanza semplici. Con i confronti fra FIFO e SJF non è sempre ovvio andare a vedere quale è il comportamento migliore perché il problema è talmente ricco che si presta ad essere misurato dal punto di vista delle prestazioni sotto diversi aspetti per cui anche algoritmi come il FIFO che uno andrebbe a scartare immediatamente in certe particolari condizioni possono anche funzionare bene. In sistemi in cui le problematiche sono tante e le esigenze sono tante, algoritmi come il FIFO sono ovviamente da scartare. Stessa cosa per SJF perché sebbene sia ottimo in termini di tempo medio e di risposta, non si presta a lavorare bene in condizione di sistemi interattivi, in sistemi a carico misto etc. Il Round Robin è stato introdotto per risolvere una serie di problematiche del SJF e del FIFO per introdurre maggiore equità, e introduce maggiore equità se i thread sono simili ovvero hanno caratteristiche simili, sotto gli aspetti di tempo di completamento medio funziona peggio degli altri, però nel suo comportamento medio in sistemi interattivi etc offre delle prestazioni migliori. Anche il Round Robin non è equo in certe condizioni di carico misto e per questo c’è necessità di soluzioni più complesse come MFQ che possiamo considerare lo stato dell’arte per sistemi general purpose. In sistemi special purpose come sistemi in tempo reale, sistemi unbeded il nostro orologio non ha una MFQ, quindi su sistemi con esigenze particolari potremmo ritrovare sistemi che in questa fase stiamo scartando.
